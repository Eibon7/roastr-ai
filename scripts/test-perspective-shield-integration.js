#!/usr/bin/env node
/**
 * Perspective API → Shield Integration Test
 *
 * Validates that toxicity analysis flows correctly from Perspective API
 * through AnalyzeToxicityWorker to Shield decision system.
 *
 * Tests 6 toxicity levels:
 * - Clean (0.00-0.39)
 * - Low (0.40-0.59)
 * - Medium (0.60-0.84)
 * - High (0.85-0.94)
 * - Critical (0.95+)
 * - Threat (severe categories)
 */

const PerspectiveService = require('../src/services/perspective');
const ShieldService = require('../src/services/shieldService');
const logger = require('../src/utils/logger');

// Test comments designed to hit different toxicity levels
const TEST_CASES = [
  {
    level: 'clean',
    expectedScore: 0.00,
    comment: 'Hello world, this is a friendly comment!',
    description: 'Benign, positive content'
  },
  {
    level: 'low',
    expectedScore: 0.30,
    comment: 'I disagree with your opinion on this topic.',
    description: 'Mild disagreement, no toxicity'
  },
  {
    level: 'medium',
    expectedScore: 0.55,
    comment: 'You are wrong and don\'t know what you\'re talking about.',
    description: 'Dismissive, mildly toxic'
  },
  {
    level: 'high',
    expectedScore: 0.80,
    comment: 'You are a complete idiot and your ideas are garbage.',
    description: 'Personal attack, insult'
  },
  {
    level: 'critical',
    expectedScore: 0.95,
    comment: 'You are a worthless piece of trash and should die.',
    description: 'Severe toxicity, dehumanization'
  },
  {
    level: 'threat',
    expectedScore: 1.00,
    comment: 'I will find you and hurt you, you deserve violence.',
    description: 'Explicit threat of violence'
  }
];

// Shield priority levels mapping (from shieldService.js)
const PRIORITY_LEVELS = {
  critical: 1,
  high: 2,
  medium: 3,
  low: 5
};

// Shield action matrix (from shieldService.js)
const ACTION_MATRIX = {
  critical: { action: 'block', escalation: 'report', auto_execute: true },
  high: { action: 'mute_permanent', escalation: 'manual_review', auto_execute: true },
  medium: { action: 'mute_temp', escalation: 'warn', auto_execute: false },
  low: { action: 'warn', escalation: null, auto_execute: false }
};

/**
 * Simulate Shield decision based on severity level
 */
function simulateShieldDecision(severity, toxicityScore, categories) {
  // Determine priority
  let priority;
  if (severity === 'critical' || toxicityScore >= 0.95) {
    priority = PRIORITY_LEVELS.critical;
  } else if (severity === 'high' || categories?.some(cat => ['threat', 'hate', 'harassment'].includes(cat))) {
    priority = PRIORITY_LEVELS.high;
  } else if (severity === 'medium' || toxicityScore >= 0.6) {
    priority = PRIORITY_LEVELS.medium;
  } else {
    priority = PRIORITY_LEVELS.low;
  }

  // Map priority to severity for action matrix
  let actionSeverity = 'low';
  if (priority === PRIORITY_LEVELS.critical) actionSeverity = 'critical';
  else if (priority === PRIORITY_LEVELS.high) actionSeverity = 'high';
  else if (priority === PRIORITY_LEVELS.medium) actionSeverity = 'medium';

  const action = ACTION_MATRIX[actionSeverity];

  return {
    priority,
    priorityLabel: Object.keys(PRIORITY_LEVELS).find(k => PRIORITY_LEVELS[k] === priority),
    action: action.action,
    escalation: action.escalation,
    autoExecute: action.auto_execute,
    moderated: priority <= PRIORITY_LEVELS.medium, // medium or higher
    manualReview: action.escalation === 'manual_review',
    autoShield: action.auto_execute
  };
}

/**
 * Run integration test
 */
async function runIntegrationTest() {
  console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');
  console.log('🧪 Perspective API → Shield Integration Test');
  console.log('━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━');
  console.log('');

  // Check if Perspective API is available
  const apiKey = process.env.PERSPECTIVE_API_KEY;
  if (!apiKey) {
    console.log('⚠️  PERSPECTIVE_API_KEY not configured');
    console.log('   Using MOCK MODE with simulated responses\n');
  } else {
    console.log('✅ Perspective API configured');
    console.log('   Using REAL API calls\n');
  }

  const perspectiveService = new PerspectiveService(apiKey);
  const results = [];

  // Test each case
  for (const testCase of TEST_CASES) {
    console.log(`\n${'─'.repeat(80)}`);
    console.log(`Testing: ${testCase.level.toUpperCase()} (${testCase.description})`);
    console.log(`Comment: "${testCase.comment}"`);
    console.log(`${'─'.repeat(80)}`);

    try {
      let analysisResult;

      if (!apiKey) {
        // Mock response based on expected level
        analysisResult = mockPerspectiveResponse(testCase);
      } else {
        // Real API call
        analysisResult = await perspectiveService.analyzeToxicity(testCase.comment, {
          languages: ['en'],
          doNotStore: true
        });

        // Add 1 second delay to respect rate limit
        await new Promise(resolve => setTimeout(resolve, 1000));
      }

      // Simulate Shield decision
      const shieldDecision = simulateShieldDecision(
        analysisResult.severity,
        analysisResult.toxicityScore,
        analysisResult.categories
      );

      // Log results
      console.log(`\n📊 Perspective API Response:`);
      console.log(`   Toxicity Score: ${analysisResult.toxicityScore.toFixed(3)}`);
      console.log(`   Severity Level: ${analysisResult.severity}`);
      console.log(`   Categories: ${analysisResult.categories.join(', ') || 'none'}`);
      console.log(`   Raw Scores:`, JSON.stringify(analysisResult.scores, null, 2).split('\n').map((l, i) => i === 0 ? l : `              ${l}`).join('\n'));

      console.log(`\n🛡️  Shield Decision:`);
      console.log(`   Priority: ${shieldDecision.priorityLabel} (${shieldDecision.priority})`);
      console.log(`   Action: ${shieldDecision.action}`);
      console.log(`   Escalation: ${shieldDecision.escalation || 'none'}`);
      console.log(`   Auto Execute: ${shieldDecision.autoExecute ? 'YES' : 'NO'}`);
      console.log(`   Moderated: ${shieldDecision.moderated ? 'YES' : 'NO'}`);
      console.log(`   Manual Review: ${shieldDecision.manualReview ? 'YES' : 'NO'}`);

      // Store result
      results.push({
        testCase,
        perspectiveResponse: analysisResult,
        shieldDecision,
        timestamp: new Date().toISOString()
      });

      // Verdict
      const expectedSeverity = getSeverityFromScore(testCase.expectedScore);
      const match = analysisResult.severity === expectedSeverity;
      console.log(`\n${match ? '✅' : '⚠️'}  Expected: ${expectedSeverity}, Got: ${analysisResult.severity}`);

    } catch (error) {
      console.error(`\n❌ Error testing ${testCase.level}:`, error.message);
      results.push({
        testCase,
        error: error.message,
        timestamp: new Date().toISOString()
      });
    }
  }

  // Summary
  console.log(`\n\n${'═'.repeat(80)}`);
  console.log('📋 TEST SUMMARY');
  console.log(`${'═'.repeat(80)}`);
  console.log(`\nTotal Tests: ${TEST_CASES.length}`);
  console.log(`Successful: ${results.filter(r => !r.error).length}`);
  console.log(`Failed: ${results.filter(r => r.error).length}`);

  // Summary table
  console.log(`\n${'─'.repeat(80)}`);
  console.log('| Level      | Score | Severity | Priority | Action          | Auto | Moderate |');
  console.log(`${'─'.repeat(80)}`);
  results.forEach(r => {
    if (r.error) {
      console.log(`| ${r.testCase.level.padEnd(10)} | ERROR | ${r.error.substring(0, 40).padEnd(40)} |`);
    } else {
      const tc = r.testCase;
      const pr = r.perspectiveResponse;
      const sd = r.shieldDecision;
      console.log(
        `| ${tc.level.padEnd(10)} | ` +
        `${pr.toxicityScore.toFixed(2)} | ` +
        `${pr.severity.padEnd(8)} | ` +
        `${sd.priorityLabel.padEnd(8)} | ` +
        `${sd.action.padEnd(15)} | ` +
        `${(sd.autoExecute ? 'YES' : 'NO').padEnd(4)} | ` +
        `${(sd.moderated ? 'YES' : 'NO').padEnd(8)} |`
      );
    }
  });
  console.log(`${'─'.repeat(80)}`);

  // Return results for evidence generation
  return results;
}

/**
 * Mock Perspective API response based on test case
 */
function mockPerspectiveResponse(testCase) {
  const scoreMap = {
    'clean': { toxicity: 0.05, severeToxicity: 0.01, threat: 0.00 },
    'low': { toxicity: 0.45, severeToxicity: 0.10, threat: 0.05 },
    'medium': { toxicity: 0.65, severeToxicity: 0.30, threat: 0.15 },
    'high': { toxicity: 0.88, severeToxicity: 0.60, threat: 0.40 },
    'critical': { toxicity: 0.97, severeToxicity: 0.95, threat: 0.70 },
    'threat': { toxicity: 0.99, severeToxicity: 0.98, threat: 0.99 }
  };

  const scores = scoreMap[testCase.level];
  const severity = getSeverityFromScores(scores.toxicity, scores.severeToxicity, scores.threat);
  const categories = getCategoriesFromLevel(testCase.level, scores);

  return {
    toxicityScore: scores.toxicity,
    severity,
    categories,
    scores: {
      toxicity: scores.toxicity,
      severeToxicity: scores.severeToxicity,
      identityAttack: scores.toxicity * 0.5,
      insult: scores.toxicity * 0.7,
      profanity: scores.toxicity * 0.6,
      threat: scores.threat
    },
    metadata: {
      textLength: testCase.comment.length,
      timestamp: new Date().toISOString(),
      provider: 'perspective-api-mock'
    }
  };
}

/**
 * Calculate severity from scores (matches perspective.js logic)
 */
function getSeverityFromScores(toxicity, severeToxicity, threat) {
  if (severeToxicity >= 0.95 || threat >= 0.95) return 'critical';
  if (toxicity >= 0.85 || severeToxicity >= 0.85 || threat >= 0.85) return 'high';
  if (toxicity >= 0.6) return 'medium';
  if (toxicity >= 0.4) return 'low';
  return 'clean';
}

/**
 * Get severity from single toxicity score
 */
function getSeverityFromScore(score) {
  if (score >= 0.95) return 'critical';
  if (score >= 0.85) return 'high';
  if (score >= 0.6) return 'medium';
  if (score >= 0.4) return 'low';
  return 'clean';
}

/**
 * Determine categories from level
 */
function getCategoriesFromLevel(level, scores) {
  const categories = [];
  if (level === 'threat' || scores.threat >= 0.7) categories.push('threat');
  if (level === 'critical' || scores.severeToxicity >= 0.7) categories.push('severe_toxicity');
  if (['high', 'critical'].includes(level) && scores.toxicity >= 0.7) categories.push('insult');
  if (level === 'threat') categories.push('profanity');
  return categories;
}

// Run test if executed directly
if (require.main === module) {
  runIntegrationTest()
    .then(results => {
      console.log('\n✅ Test completed successfully');
      console.log(`\n💾 Results available for evidence generation`);
      process.exit(0);
    })
    .catch(error => {
      console.error('\n❌ Test failed:', error);
      process.exit(1);
    });
}

module.exports = { runIntegrationTest };
