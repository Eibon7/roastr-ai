#!/usr/bin/env node

/**
 * üß™ Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona
 * 
 * Valida el flujo completo de an√°lisis de un comentario recibido usando OpenAI Moderation API,
 * y c√≥mo se modifica ese an√°lisis si el Roastr Persona est√° activo.
 * 
 * Objetivo:
 * 1. Confirmar que el sistema analiza correctamente la toxicidad del comentario.
 * 2. Confirmar que, si el Roastr Persona del usuario contiene temas sensibles relacionados,
 *    se aplica un ajuste al score de toxicidad y se etiquetan como `personal_attack`.
 */

// Configurar variables de entorno necesarias
process.env.SUPABASE_URL = 'https://mock.supabase.co';
process.env.SUPABASE_SERVICE_KEY = 'mock-service-key';
process.env.OPENAI_API_KEY = 'mock-openai-key';

const AnalyzeToxicityWorker = require('./src/workers/AnalyzeToxicityWorker');
const encryptionService = require('./src/services/encryptionService');

// Mock de Supabase para simular datos
const mockSupabase = {
  from: () => mockSupabase,
  select: () => mockSupabase,
  eq: () => mockSupabase,
  single: () => Promise.resolve({ data: { id: 'comment-123' }, error: null }),
  update: () => mockSupabase,
  insert: () => mockSupabase
};

// Mock de OpenAI Moderation API
const mockOpenAIResponse = {
  results: [{
    flagged: true,
    categories: {
      hate: true,
      harassment: true,
      'hate/threatening': false,
      'harassment/threatening': false,
      'self-harm': false,
      'self-harm/intent': false,
      'self-harm/instructions': false,
      sexual: false,
      'sexual/minors': false,
      violence: false,
      'violence/graphic': false
    },
    category_scores: {
      hate: 0.65,
      harassment: 0.72,
      'hate/threatening': 0.01,
      'harassment/threatening': 0.02,
      'self-harm': 0.001,
      'self-harm/intent': 0.001,
      'self-harm/instructions': 0.001,
      sexual: 0.001,
      'sexual/minors': 0.001,
      violence: 0.05,
      'violence/graphic': 0.001
    }
  }]
};

async function testCommentAnalysisFlow() {
  console.log('üß™ Iniciando Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona\n');

  // Configurar el worker
  const worker = new AnalyzeToxicityWorker();
  worker.supabase = mockSupabase;

  // Mock del cliente OpenAI
  worker.openaiClient = {
    moderations: {
      create: () => Promise.resolve(mockOpenAIResponse)
    }
  };

  // Comentario de prueba con contenido ligeramente t√≥xico
  const testComment = "Qu√© asco das, vuelve a tu pa√≠s";

  console.log('üìù Comentario de prueba:', testComment);
  console.log('');

  // PASO 1: An√°lisis sin Roastr Persona
  console.log('üîç PASO 1: An√°lisis sin Roastr Persona');
  console.log('=' .repeat(50));

  const resultWithoutPersona = await worker.analyzeToxicity(testComment, null);

  console.log('üìä Resultado del an√°lisis base:');
  console.log(`   - Score de toxicidad: ${resultWithoutPersona.toxicity_score}`);
  console.log(`   - Categor√≠as: ${resultWithoutPersona.categories.join(', ')}`);
  console.log(`   - Servicio usado: ${resultWithoutPersona.service}`);
  console.log(`   - Raw scores:`, JSON.stringify(resultWithoutPersona.raw_scores, null, 4));
  console.log('');

  // PASO 2: An√°lisis con Roastr Persona que contiene temas relacionados
  console.log('üîç PASO 2: An√°lisis con Roastr Persona relacionado');
  console.log('=' .repeat(50));
  
  const roastrPersona = "odio el racismo, odio los comentarios xen√≥fobos, soy inmigrante";
  console.log('üë§ Roastr Persona:', roastrPersona);
  console.log('');
  
  const resultWithPersona = await worker.analyzeToxicity(testComment, roastrPersona);
  
  console.log('üìä Resultado del an√°lisis con Roastr Persona:');
  console.log(`   - Score de toxicidad original: ${resultWithoutPersona.toxicity_score}`);
  console.log(`   - Score de toxicidad mejorado: ${resultWithPersona.toxicity_score}`);
  console.log(`   - Boost aplicado: +${(resultWithPersona.toxicity_score - resultWithoutPersona.toxicity_score).toFixed(3)}`);
  console.log(`   - Categor√≠as: ${resultWithPersona.categories.join(', ')}`);
  console.log(`   - Contiene 'personal_attack': ${resultWithPersona.categories.includes('personal_attack')}`);
  console.log('');

  // PASO 3: Verificar que el boost est√° en el rango esperado
  console.log('‚úÖ PASO 3: Verificaci√≥n de resultados');
  console.log('=' .repeat(50));
  
  const boostAmount = resultWithPersona.toxicity_score - resultWithoutPersona.toxicity_score;
  const hasPersonalAttack = resultWithPersona.categories.includes('personal_attack');
  
  console.log('üîç Verificaciones:');
  
  // Verificar que el boost est√° en el rango esperado (+0.2 a +0.6)
  if (boostAmount >= 0.2 && boostAmount <= 0.6) {
    console.log(`   ‚úÖ Boost en rango esperado: +${boostAmount.toFixed(3)} (0.2-0.6)`);
  } else {
    console.log(`   ‚ùå Boost fuera de rango: +${boostAmount.toFixed(3)} (esperado: 0.2-0.6)`);
  }
  
  // Verificar que se a√±adi√≥ la categor√≠a personal_attack
  if (hasPersonalAttack) {
    console.log('   ‚úÖ Categor√≠a "personal_attack" a√±adida correctamente');
  } else {
    console.log('   ‚ùå Categor√≠a "personal_attack" NO fue a√±adida');
  }
  
  // Verificar que el score final no excede 1.0
  if (resultWithPersona.toxicity_score <= 1.0) {
    console.log(`   ‚úÖ Score final dentro del l√≠mite: ${resultWithPersona.toxicity_score} <= 1.0`);
  } else {
    console.log(`   ‚ùå Score final excede el l√≠mite: ${resultWithPersona.toxicity_score} > 1.0`);
  }
  
  console.log('');

  // PASO 4: Simular almacenamiento del resultado
  console.log('üíæ PASO 4: Simulaci√≥n de almacenamiento');
  console.log('=' .repeat(50));
  
  // Mock de la actualizaci√≥n en base de datos
  // Ya est√° configurado en el mock de Supabase arriba
  
  const updateData = {
    toxicity_score: resultWithPersona.toxicity_score,
    severity_level: resultWithPersona.toxicity_score > 0.7 ? 'high' : 
                   resultWithPersona.toxicity_score > 0.5 ? 'medium' : 'low',
    categories: resultWithPersona.categories,
    analysis_service: resultWithPersona.service,
    analyzed_at: new Date().toISOString(),
    persona_enhanced: hasPersonalAttack
  };
  
  console.log('üìù Datos que se guardar√≠an en la base de datos:');
  console.log(JSON.stringify(updateData, null, 2));
  console.log('');

  // PASO 5: Resumen final
  console.log('üìã RESUMEN FINAL');
  console.log('=' .repeat(50));
  console.log(`‚úÖ An√°lisis base completado (score: ${resultWithoutPersona.toxicity_score})`);
  console.log(`‚úÖ An√°lisis con Roastr Persona completado (score: ${resultWithPersona.toxicity_score})`);
  console.log(`‚úÖ Boost aplicado: +${boostAmount.toFixed(3)}`);
  console.log(`‚úÖ Personal attack detectado: ${hasPersonalAttack}`);
  console.log(`‚úÖ Resultado guardado con nivel: ${updateData.severity_level}`);
  console.log('');
  console.log('üéâ Test de flujo 2 completado exitosamente!');
}

async function testAdditionalScenarios() {
  console.log('\nüß™ CASOS ADICIONALES DE PRUEBA');
  console.log('=' .repeat(50));

  const worker = new AnalyzeToxicityWorker();
  worker.supabase = mockSupabase;
  worker.openaiClient = {
    moderations: {
      create: () => Promise.resolve(mockOpenAIResponse)
    }
  };

  // Caso 1: Comentario no relacionado con el Roastr Persona
  console.log('\nüìù Caso 1: Comentario no relacionado con Roastr Persona');
  const unrelatedComment = "Tu aplicaci√≥n es una basura";
  const persona1 = "soy vegana, amo los animales";

  const result1 = await worker.analyzeToxicity(unrelatedComment, persona1);
  console.log(`   - Comentario: "${unrelatedComment}"`);
  console.log(`   - Persona: "${persona1}"`);
  console.log(`   - Score: ${result1.toxicity_score}`);
  console.log(`   - Personal attack: ${result1.categories.includes('personal_attack')}`);

  // Caso 2: M√∫ltiples t√©rminos del persona en el comentario
  console.log('\nüìù Caso 2: M√∫ltiples t√©rminos del persona');
  const multiTermComment = "Las veganas feministas son locas";
  const persona2 = "soy vegana, feminista, activista";

  const result2 = await worker.analyzeToxicity(multiTermComment, persona2);
  console.log(`   - Comentario: "${multiTermComment}"`);
  console.log(`   - Persona: "${persona2}"`);
  console.log(`   - Score: ${result2.toxicity_score}`);
  console.log(`   - Personal attack: ${result2.categories.includes('personal_attack')}`);

  // Caso 3: Comentario con slur patterns
  console.log('\nüìù Caso 3: Comentario con patrones de slur');
  const slurComment = "Las tr@nnys son raras";
  const persona3 = "mujer trans, activista LGBTQ+";

  const result3 = await worker.analyzeToxicity(slurComment, persona3);
  console.log(`   - Comentario: "${slurComment}"`);
  console.log(`   - Persona: "${persona3}"`);
  console.log(`   - Score: ${result3.toxicity_score}`);
  console.log(`   - Personal attack: ${result3.categories.includes('personal_attack')}`);

  console.log('\n‚úÖ Casos adicionales completados');
}

// Ejecutar el test si se llama directamente
if (require.main === module) {
  Promise.all([
    testCommentAnalysisFlow(),
    testAdditionalScenarios()
  ]).catch(error => {
    console.error('‚ùå Error en el test:', error.message);
    console.error(error.stack);
    process.exit(1);
  });
}

module.exports = { testCommentAnalysisFlow, testAdditionalScenarios };
