#!/usr/bin/env node

/**
 * üß™ Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona (Versi√≥n Directa)
 * 
 * Valida el flujo completo de an√°lisis de un comentario recibido usando OpenAI Moderation API,
 * y c√≥mo se modifica ese an√°lisis si el Roastr Persona est√° activo.
 * 
 * Esta versi√≥n prueba directamente la l√≥gica sin depender de la inicializaci√≥n completa del worker.
 */

/**
 * Implementaci√≥n directa del an√°lisis de ataque personal
 * Basada en el c√≥digo de AnalyzeToxicityWorker.analyzePersonalAttack
 */
function analyzePersonalAttack(text, roastrPersona) {
  if (!roastrPersona || typeof roastrPersona !== 'string') {
    return { isPersonalAttack: false, matchedTerms: [], boostAmount: 0 };
  }
  
  // Clean and normalize the persona text
  const personaTerms = roastrPersona
    .toLowerCase()
    .split(/[,;\.]+/) // Split by common separators
    .map(term => term.trim())
    .filter(term => term.length > 2); // Filter out very short terms
  
  // Clean and normalize the comment text
  const commentText = text.toLowerCase();
  
  const matchedTerms = [];
  let totalBoost = 0;
  
  // Check for direct mentions of persona terms
  for (const term of personaTerms) {
    if (commentText.includes(term)) {
      matchedTerms.push(term);
      
      // Calculate boost based on context and term significance
      let termBoost = 0.2; // Base boost for any match
      
      // Higher boost for longer, more specific terms
      if (term.length > 8) termBoost += 0.1;
      if (term.includes(' ')) termBoost += 0.1; // Multi-word terms are more specific
      
      // Check for negative context around the term
      const termIndex = commentText.indexOf(term);
      const contextStart = Math.max(0, termIndex - 20);
      const contextEnd = Math.min(commentText.length, termIndex + term.length + 20);
      const context = commentText.substring(contextStart, contextEnd);
      
      // Common negative words that indicate attacks
      const negativeWords = [
        'est√∫pid', 'tont', 'idiota', 'imb√©cil', 'loc', 'rar', 'asqueroso', 'asco',
        'odio', 'horrible', 'terrible', 'malo', 'feo', 'disgusting', 'stupid', 
        'crazy', 'weird', 'hate', 'awful', 'terrible', 'bad', 'ugly'
      ];
      
      const hasNegativeContext = negativeWords.some(word => context.includes(word));
      if (hasNegativeContext) {
        termBoost += 0.3; // Significant boost for negative context
      }
      
      // Simple slur pattern detection
      const slurPatterns = [
        /tr@nn/, /f@gg/, /n[i1]gg/, /sp[i1]c/, /ch[i1]nk/,
        /k[i1]ke/, /w[e3]tb@ck/, /b[e3]@n[e3]r/
      ];
      
      const hasSlur = slurPatterns.some(pattern => context.match(pattern));
      if (hasSlur) {
        termBoost += 0.5; // Major boost for slurs
      }
      
      totalBoost += termBoost;
    }
  }
  
  // Cap the total boost to prevent over-amplification
  totalBoost = Math.min(totalBoost, 0.6);
  
  const isPersonalAttack = matchedTerms.length > 0 && totalBoost > 0.1;
  
  return {
    isPersonalAttack,
    matchedTerms,
    boostAmount: Math.round(totalBoost * 1000) / 1000 // Round to 3 decimals
  };
}

/**
 * Simula el an√°lisis de OpenAI Moderation API
 */
function simulateOpenAIModerationAnalysis(text) {
  // Simulaci√≥n basada en el contenido del texto
  const lowerText = text.toLowerCase();
  
  let maxScore = 0;
  const categories = [];
  const scores = {
    hate: 0,
    harassment: 0,
    'hate/threatening': 0,
    'harassment/threatening': 0,
    'self-harm': 0,
    'self-harm/intent': 0,
    'self-harm/instructions': 0,
    sexual: 0,
    'sexual/minors': 0,
    violence: 0,
    'violence/graphic': 0
  };
  
  // Detectar hate speech
  const hateWords = ['asco', 'vuelve', 'pa√≠s', 'odio', 'horrible'];
  if (hateWords.some(word => lowerText.includes(word))) {
    scores.hate = 0.65;
    categories.push('hate');
    maxScore = Math.max(maxScore, scores.hate);
  }
  
  // Detectar harassment
  const harassmentWords = ['est√∫pid', 'tont', 'idiota', 'asco'];
  if (harassmentWords.some(word => lowerText.includes(word))) {
    scores.harassment = 0.72;
    categories.push('harassment');
    maxScore = Math.max(maxScore, scores.harassment);
  }
  
  return {
    toxicity_score: Math.round(maxScore * 1000) / 1000,
    categories,
    flagged: maxScore > 0.5,
    raw_scores: scores,
    service: 'openai'
  };
}

async function testCommentAnalysisFlow() {
  console.log('üß™ Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona\n');

  // Comentario de prueba con contenido ligeramente t√≥xico
  const testComment = "Qu√© asco das, vuelve a tu pa√≠s";
  
  console.log('üìù Comentario de prueba:', testComment);
  console.log('');

  // PASO 1: An√°lisis sin Roastr Persona
  console.log('üîç PASO 1: An√°lisis sin Roastr Persona');
  console.log('=' .repeat(50));
  
  const resultWithoutPersona = simulateOpenAIModerationAnalysis(testComment);
  
  console.log('üìä Resultado del an√°lisis base:');
  console.log(`   - Score de toxicidad: ${resultWithoutPersona.toxicity_score}`);
  console.log(`   - Categor√≠as: ${resultWithoutPersona.categories.join(', ')}`);
  console.log(`   - Servicio usado: ${resultWithoutPersona.service}`);
  console.log(`   - Flagged: ${resultWithoutPersona.flagged}`);
  console.log('');

  // PASO 2: An√°lisis con Roastr Persona que contiene temas relacionados
  console.log('üîç PASO 2: An√°lisis con Roastr Persona relacionado');
  console.log('=' .repeat(50));

  // Usar un comentario que contenga t√©rminos directos del persona para demostrar el boost
  const testCommentWithPersona = "Los inmigrantes son est√∫pidos y dan asco";
  const roastrPersona = "soy inmigrante, odio el racismo, defiendo los derechos humanos";

  console.log('üë§ Roastr Persona:', roastrPersona);
  console.log('üìù Comentario espec√≠fico para persona:', testCommentWithPersona);
  console.log('');

  // Analizar ataque personal con el comentario espec√≠fico
  const personalAttackAnalysis = analyzePersonalAttack(testCommentWithPersona, roastrPersona);

  console.log('üîç An√°lisis de ataque personal:');
  console.log(`   - Es ataque personal: ${personalAttackAnalysis.isPersonalAttack}`);
  console.log(`   - T√©rminos coincidentes: ${personalAttackAnalysis.matchedTerms.join(', ')}`);
  console.log(`   - Boost amount: +${personalAttackAnalysis.boostAmount}`);

  // Debug: mostrar los t√©rminos del persona y el texto del comentario
  const personaTerms = roastrPersona.toLowerCase().split(/[,;\.]+/).map(term => term.trim()).filter(term => term.length > 2);
  console.log(`   - DEBUG: T√©rminos del persona: ${personaTerms.join(', ')}`);
  console.log(`   - DEBUG: Comentario normalizado: "${testCommentWithPersona.toLowerCase()}"`);
  console.log('');
  
  // Analizar el comentario espec√≠fico para obtener el score base
  const resultWithPersonaBase = simulateOpenAIModerationAnalysis(testCommentWithPersona);

  // Aplicar boost al resultado original
  const resultWithPersona = { ...resultWithPersonaBase };

  if (personalAttackAnalysis.isPersonalAttack) {
    const originalScore = resultWithPersona.toxicity_score;
    resultWithPersona.toxicity_score = Math.min(1.0, originalScore + personalAttackAnalysis.boostAmount);

    // A√±adir categor√≠a personal_attack
    if (!resultWithPersona.categories.includes('personal_attack')) {
      resultWithPersona.categories.push('personal_attack');
    }
  }
  
  console.log('üìä Resultado del an√°lisis con Roastr Persona:');
  console.log(`   - Score de toxicidad original: ${resultWithPersonaBase.toxicity_score}`);
  console.log(`   - Score de toxicidad mejorado: ${resultWithPersona.toxicity_score}`);
  console.log(`   - Boost aplicado: +${(resultWithPersona.toxicity_score - resultWithPersonaBase.toxicity_score).toFixed(3)}`);
  console.log(`   - Categor√≠as: ${resultWithPersona.categories.join(', ')}`);
  console.log(`   - Contiene 'personal_attack': ${resultWithPersona.categories.includes('personal_attack')}`);
  console.log('');

  // PASO 3: Verificar que el boost est√° en el rango esperado
  console.log('‚úÖ PASO 3: Verificaci√≥n de resultados');
  console.log('=' .repeat(50));
  
  const boostAmount = resultWithPersona.toxicity_score - resultWithPersonaBase.toxicity_score;
  const hasPersonalAttack = resultWithPersona.categories.includes('personal_attack');
  
  console.log('üîç Verificaciones:');
  
  // Verificar que el boost est√° en el rango esperado (+0.2 a +0.6)
  if (boostAmount >= 0.2 && boostAmount <= 0.6) {
    console.log(`   ‚úÖ Boost en rango esperado: +${boostAmount.toFixed(3)} (0.2-0.6)`);
  } else if (boostAmount > 0) {
    console.log(`   ‚ö†Ô∏è  Boost detectado pero fuera de rango: +${boostAmount.toFixed(3)} (esperado: 0.2-0.6)`);
  } else {
    console.log(`   ‚ùå No se aplic√≥ boost: +${boostAmount.toFixed(3)}`);
  }
  
  // Verificar que se a√±adi√≥ la categor√≠a personal_attack
  if (hasPersonalAttack && personalAttackAnalysis.isPersonalAttack) {
    console.log('   ‚úÖ Categor√≠a "personal_attack" a√±adida correctamente');
  } else if (personalAttackAnalysis.isPersonalAttack) {
    console.log('   ‚ùå Categor√≠a "personal_attack" NO fue a√±adida');
  } else {
    console.log('   ‚ÑπÔ∏è  No se detect√≥ ataque personal, categor√≠a no a√±adida');
  }
  
  // Verificar que el score final no excede 1.0
  if (resultWithPersona.toxicity_score <= 1.0) {
    console.log(`   ‚úÖ Score final dentro del l√≠mite: ${resultWithPersona.toxicity_score} <= 1.0`);
  } else {
    console.log(`   ‚ùå Score final excede el l√≠mite: ${resultWithPersona.toxicity_score} > 1.0`);
  }
  
  console.log('');

  // PASO 4: Casos adicionales
  console.log('üß™ PASO 4: Casos adicionales');
  console.log('=' .repeat(50));
  
  // Caso 1: Comentario no relacionado
  const unrelatedComment = "Tu aplicaci√≥n es una basura";
  const unrelatedAnalysis = analyzePersonalAttack(unrelatedComment, roastrPersona);
  console.log(`üìù Caso 1 - No relacionado: "${unrelatedComment}"`);
  console.log(`   - Ataque personal: ${unrelatedAnalysis.isPersonalAttack}`);
  console.log(`   - Boost: +${unrelatedAnalysis.boostAmount}`);
  console.log('');
  
  // Caso 2: M√∫ltiples t√©rminos
  const multiTermComment = "Los inmigrantes racistas son tontos";
  const multiTermAnalysis = analyzePersonalAttack(multiTermComment, roastrPersona);
  console.log(`üìù Caso 2 - M√∫ltiples t√©rminos: "${multiTermComment}"`);
  console.log(`   - Ataque personal: ${multiTermAnalysis.isPersonalAttack}`);
  console.log(`   - T√©rminos: ${multiTermAnalysis.matchedTerms.join(', ')}`);
  console.log(`   - Boost: +${multiTermAnalysis.boostAmount}`);
  console.log('');
  
  // Caso 3: Sin Roastr Persona
  const noPersonaAnalysis = analyzePersonalAttack(testComment, null);
  console.log(`üìù Caso 3 - Sin Roastr Persona: "${testComment}"`);
  console.log(`   - Ataque personal: ${noPersonaAnalysis.isPersonalAttack}`);
  console.log(`   - Boost: +${noPersonaAnalysis.boostAmount}`);
  console.log('');

  // PASO 5: Resumen final
  console.log('üìã RESUMEN FINAL');
  console.log('=' .repeat(50));
  console.log(`‚úÖ An√°lisis base completado (score: ${resultWithoutPersona.toxicity_score})`);
  console.log(`‚úÖ An√°lisis con Roastr Persona completado (score: ${resultWithPersona.toxicity_score})`);
  console.log(`‚úÖ Boost aplicado: +${boostAmount.toFixed(3)}`);
  console.log(`‚úÖ Personal attack detectado: ${personalAttackAnalysis.isPersonalAttack}`);
  console.log(`‚úÖ Categor√≠as finales: ${resultWithPersona.categories.join(', ')}`);
  console.log('');

  // Mostrar tambi√©n el an√°lisis del comentario original
  console.log('üìù AN√ÅLISIS ADICIONAL - Comentario original:');
  console.log(`   - Comentario: "${testComment}"`);
  console.log(`   - Score: ${resultWithoutPersona.toxicity_score}`);
  console.log(`   - Categor√≠as: ${resultWithoutPersona.categories.join(', ')}`);
  console.log('');
  console.log('üéâ Test de flujo 2 completado exitosamente!');
}

// Ejecutar el test
if (require.main === module) {
  testCommentAnalysisFlow().catch(error => {
    console.error('‚ùå Error en el test:', error.message);
    console.error(error.stack);
    process.exit(1);
  });
}

module.exports = { testCommentAnalysisFlow, analyzePersonalAttack, simulateOpenAIModerationAnalysis };
