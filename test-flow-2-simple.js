#!/usr/bin/env node

/**
 * üß™ Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona (Versi√≥n Simplificada)
 * 
 * Valida el flujo completo de an√°lisis de un comentario recibido usando OpenAI Moderation API,
 * y c√≥mo se modifica ese an√°lisis si el Roastr Persona est√° activo.
 */

// Configurar variables de entorno necesarias
process.env.SUPABASE_URL = 'https://mock.supabase.co';
process.env.SUPABASE_SERVICE_KEY = 'mock-service-key';
process.env.OPENAI_API_KEY = 'mock-openai-key';

const AnalyzeToxicityWorker = require('./src/workers/AnalyzeToxicityWorker');

// Mock de OpenAI Moderation API response
const mockOpenAIResponse = {
  results: [{
    flagged: true,
    categories: {
      hate: true,
      harassment: true,
      'hate/threatening': false,
      'harassment/threatening': false,
      'self-harm': false,
      'self-harm/intent': false,
      'self-harm/instructions': false,
      sexual: false,
      'sexual/minors': false,
      violence: false,
      'violence/graphic': false
    },
    category_scores: {
      hate: 0.65,
      harassment: 0.72,
      'hate/threatening': 0.01,
      'harassment/threatening': 0.02,
      'self-harm': 0.001,
      'self-harm/intent': 0.001,
      'self-harm/instructions': 0.001,
      sexual: 0.001,
      'sexual/minors': 0.001,
      violence: 0.05,
      'violence/graphic': 0.001
    }
  }]
};

async function testPersonalAttackAnalysis() {
  console.log('üß™ Test de flujo 2 ‚Äì An√°lisis del comentario con Moderation + Roastr Persona\n');

  // Crear una instancia del worker solo para usar sus m√©todos de an√°lisis
  const worker = new AnalyzeToxicityWorker();
  
  // Comentario de prueba con contenido ligeramente t√≥xico
  const testComment = "Qu√© asco das, vuelve a tu pa√≠s";
  
  console.log('üìù Comentario de prueba:', testComment);
  console.log('');

  // PASO 1: Simular an√°lisis base con OpenAI Moderation
  console.log('üîç PASO 1: An√°lisis base con OpenAI Moderation');
  console.log('=' .repeat(50));
  
  // Simular el resultado de OpenAI Moderation
  const baseResult = {
    toxicity_score: 0.72, // Score m√°s alto de las categor√≠as
    categories: ['hate', 'harassment'],
    flagged: true,
    raw_scores: mockOpenAIResponse.results[0].category_scores,
    service: 'openai'
  };
  
  console.log('üìä Resultado del an√°lisis base:');
  console.log(`   - Score de toxicidad: ${baseResult.toxicity_score}`);
  console.log(`   - Categor√≠as: ${baseResult.categories.join(', ')}`);
  console.log(`   - Servicio usado: ${baseResult.service}`);
  console.log(`   - Flagged: ${baseResult.flagged}`);
  console.log('');

  // PASO 2: An√°lisis de ataque personal con Roastr Persona
  console.log('üîç PASO 2: An√°lisis de ataque personal con Roastr Persona');
  console.log('=' .repeat(50));
  
  const roastrPersona = "odio el racismo, odio los comentarios xen√≥fobos, soy inmigrante";
  console.log('üë§ Roastr Persona:', roastrPersona);
  console.log('');
  
  // Usar el m√©todo analyzePersonalAttack del worker
  const personalAttackAnalysis = worker.analyzePersonalAttack(testComment, roastrPersona);
  
  console.log('üîç An√°lisis de ataque personal:');
  console.log(`   - Es ataque personal: ${personalAttackAnalysis.isPersonalAttack}`);
  console.log(`   - T√©rminos coincidentes: ${personalAttackAnalysis.matchedTerms.join(', ')}`);
  console.log(`   - Boost amount: +${personalAttackAnalysis.boostAmount}`);
  console.log('');

  // PASO 3: Aplicar el boost al score original
  console.log('üîç PASO 3: Aplicaci√≥n del boost');
  console.log('=' .repeat(50));
  
  let enhancedResult = { ...baseResult };
  
  if (personalAttackAnalysis.isPersonalAttack) {
    const originalScore = enhancedResult.toxicity_score;
    enhancedResult.toxicity_score = Math.min(1.0, originalScore + personalAttackAnalysis.boostAmount);
    
    // A√±adir categor√≠a personal_attack
    if (!enhancedResult.categories.includes('personal_attack')) {
      enhancedResult.categories.push('personal_attack');
    }
    
    console.log('üìä Resultado con Roastr Persona:');
    console.log(`   - Score original: ${originalScore}`);
    console.log(`   - Score mejorado: ${enhancedResult.toxicity_score}`);
    console.log(`   - Boost aplicado: +${personalAttackAnalysis.boostAmount}`);
    console.log(`   - Categor√≠as: ${enhancedResult.categories.join(', ')}`);
    console.log(`   - Contiene 'personal_attack': ${enhancedResult.categories.includes('personal_attack')}`);
  } else {
    console.log('   - No se detect√≥ ataque personal, score sin cambios');
  }
  console.log('');

  // PASO 4: Verificaciones
  console.log('‚úÖ PASO 4: Verificaci√≥n de resultados');
  console.log('=' .repeat(50));
  
  const boostAmount = enhancedResult.toxicity_score - baseResult.toxicity_score;
  const hasPersonalAttack = enhancedResult.categories.includes('personal_attack');
  
  console.log('üîç Verificaciones:');
  
  // Verificar que el boost est√° en el rango esperado (+0.2 a +0.6)
  if (boostAmount >= 0.2 && boostAmount <= 0.6) {
    console.log(`   ‚úÖ Boost en rango esperado: +${boostAmount.toFixed(3)} (0.2-0.6)`);
  } else if (boostAmount > 0) {
    console.log(`   ‚ö†Ô∏è  Boost detectado pero fuera de rango: +${boostAmount.toFixed(3)} (esperado: 0.2-0.6)`);
  } else {
    console.log(`   ‚ùå No se aplic√≥ boost: +${boostAmount.toFixed(3)}`);
  }
  
  // Verificar que se a√±adi√≥ la categor√≠a personal_attack
  if (hasPersonalAttack && personalAttackAnalysis.isPersonalAttack) {
    console.log('   ‚úÖ Categor√≠a "personal_attack" a√±adida correctamente');
  } else if (personalAttackAnalysis.isPersonalAttack) {
    console.log('   ‚ùå Categor√≠a "personal_attack" NO fue a√±adida');
  } else {
    console.log('   ‚ÑπÔ∏è  No se detect√≥ ataque personal, categor√≠a no a√±adida');
  }
  
  // Verificar que el score final no excede 1.0
  if (enhancedResult.toxicity_score <= 1.0) {
    console.log(`   ‚úÖ Score final dentro del l√≠mite: ${enhancedResult.toxicity_score} <= 1.0`);
  } else {
    console.log(`   ‚ùå Score final excede el l√≠mite: ${enhancedResult.toxicity_score} > 1.0`);
  }
  
  console.log('');

  // PASO 5: Casos adicionales
  console.log('üß™ PASO 5: Casos adicionales');
  console.log('=' .repeat(50));
  
  // Caso 1: Comentario no relacionado
  const unrelatedComment = "Tu aplicaci√≥n es una basura";
  const unrelatedAnalysis = worker.analyzePersonalAttack(unrelatedComment, roastrPersona);
  console.log(`üìù Caso 1 - No relacionado: "${unrelatedComment}"`);
  console.log(`   - Ataque personal: ${unrelatedAnalysis.isPersonalAttack}`);
  console.log(`   - Boost: +${unrelatedAnalysis.boostAmount}`);
  console.log('');
  
  // Caso 2: M√∫ltiples t√©rminos
  const multiTermComment = "Los inmigrantes racistas son tontos";
  const multiTermAnalysis = worker.analyzePersonalAttack(multiTermComment, roastrPersona);
  console.log(`üìù Caso 2 - M√∫ltiples t√©rminos: "${multiTermComment}"`);
  console.log(`   - Ataque personal: ${multiTermAnalysis.isPersonalAttack}`);
  console.log(`   - T√©rminos: ${multiTermAnalysis.matchedTerms.join(', ')}`);
  console.log(`   - Boost: +${multiTermAnalysis.boostAmount}`);
  console.log('');

  // PASO 6: Resumen final
  console.log('üìã RESUMEN FINAL');
  console.log('=' .repeat(50));
  console.log(`‚úÖ An√°lisis base completado (score: ${baseResult.toxicity_score})`);
  console.log(`‚úÖ An√°lisis con Roastr Persona completado (score: ${enhancedResult.toxicity_score})`);
  console.log(`‚úÖ Boost aplicado: +${boostAmount.toFixed(3)}`);
  console.log(`‚úÖ Personal attack detectado: ${personalAttackAnalysis.isPersonalAttack}`);
  console.log(`‚úÖ Categor√≠as finales: ${enhancedResult.categories.join(', ')}`);
  console.log('');
  console.log('üéâ Test de flujo 2 completado exitosamente!');
}

// Ejecutar el test
if (require.main === module) {
  testPersonalAttackAnalysis().catch(error => {
    console.error('‚ùå Error en el test:', error.message);
    console.error(error.stack);
    process.exit(1);
  });
}

module.exports = { testPersonalAttackAnalysis };
